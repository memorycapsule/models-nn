{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTGv3vLejNyn"
      },
      "outputs": [],
      "source": [
        "#Scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn import svm\n",
        "#NLKT\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from collections import defaultdict\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "#Pandas\n",
        "import pandas as pd\n",
        "#Numpy\n",
        "import numpy as np\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "#Plotting\n",
        "import seaborn\n",
        "import matplotlib.pyplot as plt\n",
        "#regex\n",
        "import re\n",
        "##Load dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "twitter = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/labeled_data.csv', header=0,encoding_errors='ignore')\n",
        "twitter = twitter.replace('n/a',np.NaN)\n",
        "twitter.dropna(inplace=True)\n",
        "\n",
        "\"\"\"\n",
        "please change the location of the dataset if you want to run the code.\n",
        "You can also put the dataset in your colab folder in google drive.\n",
        "The dataset is in a folder called /datasets/.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "twitter.shape\n",
        "twitter['class'].hist()"
      ],
      "metadata": {
        "id": "ur0YtwRNjtfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "twitter['tweet'] = [entry.lower() for entry in twitter['tweet']]\n",
        "pattern = re.compile('\\W+')\n",
        "twitter['tweet'] = twitter['tweet'].str.replace(pattern,\" \")\n",
        "twitter['tweet'] = twitter['tweet'].str.replace(\"rt\",\" \")\n",
        "twitter['tweet'] = twitter['tweet'].str.replace(\"amp\",\" \")\n",
        "twitter['tweet'] = twitter['tweet'].str.replace(re.compile(r'\\b\\d+(?:\\.\\d+)?\\s+'),\"\")\n",
        "twitter['tweet'] = twitter['tweet'].str.strip('0123456789.- ')\n",
        "twitter['tweet'] = twitter['tweet'].str.strip('.-0123456789 ')\n",
        "nltk.download('stopwords')\n",
        "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
        "twitter['tweet'] = twitter['tweet'].replace('stopwords','')\n",
        "print(stopwords)"
      ],
      "metadata": {
        "id": "FXEsmpqEjzhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "#Tokenizing and stemming of tweets\n",
        "\n",
        "def stemtweet(tweet):\n",
        "  ps = PorterStemmer()\n",
        "  tokens = word_tokenize(tweet)\n",
        "  stemmed = []\n",
        "  for token in tokens:\n",
        "    stemmed_word = ps.stem(token)\n",
        "    stemmed.append(stemmed_word)\n",
        "  return stemmed\n",
        "\"\"\"\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html\n",
        "\"\"\"\n",
        "tfidf_vectorizer = TfidfVectorizer(tokenizer=stemtweet,ngram_range=(1, 2),stop_words='english',use_idf=True,smooth_idf=False,norm=None,decode_error='replace',max_features=10000,min_df=5,max_df=0.75)\n",
        "transf = tfidf_vectorizer.fit(twitter['tweet'])\n",
        "X = transf.transform(twitter['tweet'])\n",
        "y = twitter['class'].astype(int)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html\n",
        "\"\"\"\n",
        "# To use CountVectorizer, uncomment the code below, and comment the code above.\n",
        "# vectorizer = CountVectorizer(tokenizer=stem,stop_words='english', ngram_range=(1, 2),max_features=10000,min_df=5,max_df=0.75,decode_error='replace')\n",
        "# X = vectorizer.fit_transform(twitter['tweet'])\n",
        "# y = twitter['class'].astype(int)\n",
        "\n",
        "pickle.dump(transf, open(\"tfidf_vect.pickle\", \"wb\"))\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)"
      ],
      "metadata": {
        "id": "Kt-VA0RAZIVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Logistic Regression Default Model\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "\"\"\"\n",
        "model = LogisticRegression(random_state=42).fit(X_train_tfidf,y_train)\n",
        "prediction = model.predict(X_test_tfidf)\n",
        "LRACC = accuracy_score(y_test,prediction)\n",
        "print(classification_report( y_test, prediction))\n",
        "print(accuracy_score(y_test,prediction))"
      ],
      "metadata": {
        "id": "h-mmnPN-kQ33"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Random Forest Default Model\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
        "\"\"\"\n",
        "rf=RandomForestClassifier(random_state=42)\n",
        "rf.fit(X_train_tfidf,y_train)\n",
        "y_preds = rf.predict(X_test_tfidf)\n",
        "RFACC = accuracy_score(y_test,y_preds)\n",
        "print(classification_report( y_test, y_preds))\n",
        "print(accuracy_score(y_test,y_preds))"
      ],
      "metadata": {
        "id": "LhwqDaJ6kSU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Gaussian Naive Bayes Default Model\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html\n",
        "\"\"\"\n",
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X.toarray(), y, random_state=42, test_size=0.2)\n",
        "naive=GaussianNB()\n",
        "naive.fit(X_train_tfidf,y_train)\n",
        "y_preds = naive.predict(X_test_tfidf)\n",
        "NBACC = accuracy_score(y_test,y_preds)\n",
        "print(classification_report( y_test, y_preds))\n",
        "print(accuracy_score(y_test,y_preds))"
      ],
      "metadata": {
        "id": "zs8LirijkT7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Linear SVC Default Model\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html\n",
        "\"\"\"\n",
        "linear = LinearSVC(random_state=42)\n",
        "linear.fit(X_train_tfidf,y_train)\n",
        "y_preds = linear.predict(X_test_tfidf)\n",
        "L_SVC_ACC = accuracy_score(y_test,y_preds)\n",
        "print(classification_report( y_test, y_preds))\n",
        "print(accuracy_score(y_test,y_preds))"
      ],
      "metadata": {
        "id": "tpoWp7bckVWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "MLP classifier\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
        "\"\"\"\n",
        "clf = MLPClassifier(random_state=42)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "y_preds = clf.predict(X_test_tfidf)\n",
        "Perceptron_ACC = accuracy_score(y_test,y_preds)\n",
        "print(classification_report( y_test, y_preds))\n",
        "print(accuracy_score(y_test,y_preds))"
      ],
      "metadata": {
        "id": "BzevEtclJw4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "SGD classifier\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
        "\"\"\"\n",
        "clf = SGDClassifier(random_state=42)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "y_preds = clf.predict(X_test_tfidf)\n",
        "SGD_SVM_ACC = accuracy_score(y_test,y_preds)\n",
        "print(classification_report( y_test, y_preds))\n",
        "print(accuracy_score(y_test,y_preds))"
      ],
      "metadata": {
        "id": "C3vDaOLSKClo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Comparisons of different machine learning algorithms\n",
        "\"\"\"\n",
        "import matplotlib.pyplot as plt\n",
        "objects = ('Logistic', 'RF', 'NB', 'Linear', 'LP', 'SGD')\n",
        "y_pos = np.arange(len(objects))\n",
        "performance = [LRACC,RFACC,NBACC,L_SVC_ACC,Perceptron_ACC,SGD_SVM_ACC]\n",
        "plt.bar(y_pos, performance, alpha=1,align='edge')\n",
        "plt.xticks(y_pos, objects)\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Algorithm Comparision')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DnIOAMDGkW-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that the grid search functions take a very long amount of time to run as they give the most optimum parameters"
      ],
      "metadata": {
        "id": "0Ez9EqDp_m5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(rf.get_params())"
      ],
      "metadata": {
        "id": "AEqRbh2VROsc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "param_grid = {\n",
        "    'n_estimators': [50,100,150,200,250,300,350],\n",
        "    'max_features': ['auto', 'sqrt','log2'],\n",
        "    'max_depth': [5,10,20,30,40,50],\n",
        "    'bootstrap': [True, False],\n",
        "    'criterion': ['gini', 'entropy', 'MSE']\n",
        "    }"
      ],
      "metadata": {
        "id": "AJj-Aoi3UZD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X, y, random_state=42, test_size=0.2)\n",
        "CV_rfc = GridSearchCV(estimator=rf, param_grid=param_grid, cv= 5,n_jobs=-1)\n",
        "CV_rfc.fit(X_train_tfidf, y_train)\n",
        "CV_rfc.best_params_"
      ],
      "metadata": {
        "id": "Ekf6f8D3UgYf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(LinearSVC().get_params())"
      ],
      "metadata": {
        "id": "JI46YkTkmuLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tuning linear SVC\n",
        "grid search only works on svm, \n",
        "so I use linear kernel to get approximate parameters\n",
        "\"\"\"\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "    'kernel':['linear'] \n",
        "    }\n",
        "cv_SVC = GridSearchCV(svm.SVC(), param_grid, refit = True, verbose = 3)\n",
        "cv_SVC.fit(X_train_tfidf, y_train)\n",
        "cv_SVC.best_params_"
      ],
      "metadata": {
        "id": "UpYwOLCFm1Cv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Tuning logistic regression\n",
        "\"\"\"\n",
        "print(LogisticRegression().get_params())\n",
        "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000] }\n",
        "cv_LR = GridSearchCV(cv=None,\n",
        "             estimator=LogisticRegression(penalty='l2', max_iter = 10000),\n",
        "             param_grid={'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]})\n",
        "cv_LR.fit(X_train_tfidf, y_train)\n",
        "cv_LR.best_params_"
      ],
      "metadata": {
        "id": "5fD_qCp2qTtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "https://stackoverflow.com/questions/39828535/how-to-tune-gaussiannb\n",
        "For tuning gaussian NB, the hyperparameters were taken from user \"ana\" on StackOverFlow\n",
        "this can be seen in the line \"param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\"\n",
        "\"\"\"\n",
        "print(GaussianNB().get_params())\n",
        "param_grid = {'var_smoothing': np.logspace(0,-9, num=100)}\n",
        "cv_NB = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid, verbose=1, cv=10, n_jobs=-1)\n",
        "cv_NB.fit(X_train_tfidf.toarray(), y_train)\n",
        "cv_NB.best_params_"
      ],
      "metadata": {
        "id": "g_5QTM4vsiXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The optimized models are run below to get their new accuracy scores."
      ],
      "metadata": {
        "id": "gDwMIPWgCdXC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rfOptimized = RandomForestClassifier(bootstrap=False,criterion='gini',max_depth=100,max_features='sqrt',n_estimators=100)\n",
        "rfOptimized.fit(X_train_tfidf, y_train)\n",
        "y_preds_rf = rfOptimized.predict(X_test_tfidf)\n",
        "RF_OPT =accuracy_score(y_test,y_preds_rf)\n",
        "print(RF_OPT)"
      ],
      "metadata": {
        "id": "78ZKqh9fgMFO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "linearOptimized = LinearSVC(C=0.001,random_state=42)\n",
        "linearOptimized.fit(X_train_tfidf,y_train)\n",
        "y_preds_svc = linearOptimized.predict(X_test_tfidf)\n",
        "SVC_OPT = accuracy_score(y_test,y_preds_svc)\n",
        "print(SVC_OPT)"
      ],
      "metadata": {
        "id": "p0fK7LrUg7rY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regressionOptimized = LogisticRegression(C=0.01,random_state=42)\n",
        "regressionOptimized.fit(X_train_tfidf,y_train)\n",
        "y_preds_svc = regressionOptimized.predict(X_test_tfidf)\n",
        "LR_OPT = accuracy_score(y_test,y_preds_svc)\n",
        "print(LR_OPT)"
      ],
      "metadata": {
        "id": "MbkMif8_iEfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X.toarray(), y, random_state=42, test_size=0.2)\n",
        "NBOptimzed = GaussianNB(var_smoothing= 0.0002848035868435802) \n",
        "NBOptimzed.fit(X_train_tfidf,y_train)\n",
        "y_preds_NB = NBOptimzed.predict(X_test_tfidf)\n",
        "NB_OPT = accuracy_score(y_test,y_preds_NB)\n",
        "print(NB_OPT)"
      ],
      "metadata": {
        "id": "S2YcPC3niLOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Pickle stores models as a file so that they can be run elsewhere.\n",
        "\"\"\"\n",
        "# import pickle\n",
        "# pickle.dump(linearOptimized, open('lsvc.pkl', 'wb'))\n",
        "# pickle.dump(NBOptimzed, open('nb.pkl', 'wb'))\n",
        "# pickle.dump(rfOptimized, open('rf.pkl', 'wb'))\n",
        "# pickle.dump(regressionOptimized, open('lr.pkl', 'wb'))"
      ],
      "metadata": {
        "id": "g9Ek4fZSX3qJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Graph showing comparisons of four models.\n",
        "Optimized vs Unoptimized. \n",
        "\"\"\"\n",
        "y1 = [LRACC, RFACC, NBACC, L_SVC_ACC]\n",
        "y2 = [LR_OPT, RF_OPT, NB_OPT, SVC_OPT]\n",
        "width = 0.2\n",
        "x = np.arange(4)\n",
        "plt.bar(x, y1, width, color='orange')\n",
        "plt.bar(x+0.2, y2, width, color='green')\n",
        "plt.xticks(x, ['LR', 'RF', 'NB', 'SVC'])\n",
        "plt.xlabel(\"Models\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend([\"Unoptimized\", \"Optimized\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VEBKiL42ls6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "https://scikit-learn.org/stable/auto_examples/model_selection/plot_validation_curve.html\n",
        "This code follows the guide to plot a validation curve from scikit-learn\n",
        "\"\"\"\n",
        "from sklearn.model_selection import validation_curve\n",
        "subset_mask = np.isin(y, [0, 1, 2])  # binary classification: 1 vs 2\n",
        "X, y = X[subset_mask], y[subset_mask]\n",
        "\n",
        "param_range = np.logspace(-6, 1, 5)\n",
        "train_scores, test_scores = validation_curve(\n",
        "    LinearSVC(),\n",
        "    X,\n",
        "    y,\n",
        "    param_name=\"C\",\n",
        "    param_range=param_range,\n",
        "    scoring=\"accuracy\",\n",
        "    n_jobs=2,\n",
        ")\n",
        "train_scores_mean = np.mean(train_scores, axis=1)\n",
        "train_scores_std = np.std(train_scores, axis=1)\n",
        "test_scores_mean = np.mean(test_scores, axis=1)\n",
        "test_scores_std = np.std(test_scores, axis=1)\n",
        "\n",
        "plt.title(\"Validation Curve with Linear SVC\")\n",
        "plt.xlabel(r\"$\\gamma$\")\n",
        "plt.ylabel(\"Score\")\n",
        "plt.ylim(0.0, 1.1)\n",
        "lw = 2\n",
        "plt.semilogx(\n",
        "    param_range, train_scores_mean, label=\"Training score\", color=\"darkorange\", lw=lw\n",
        ")\n",
        "plt.fill_between(\n",
        "    param_range,\n",
        "    train_scores_mean - train_scores_std,\n",
        "    train_scores_mean + train_scores_std,\n",
        "    alpha=0.2,\n",
        "    color=\"darkorange\",\n",
        "    lw=lw,\n",
        ")\n",
        "plt.semilogx(\n",
        "    param_range, test_scores_mean, label=\"Cross-validation score\", color=\"navy\", lw=lw\n",
        ")\n",
        "plt.fill_between(\n",
        "    param_range,\n",
        "    test_scores_mean - test_scores_std,\n",
        "    test_scores_mean + test_scores_std,\n",
        "    alpha=0.2,\n",
        "    color=\"navy\",\n",
        "    lw=lw,\n",
        ")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZejkLUGlhrvO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        " You can test the model output by running any text within the np.array. \n",
        "The model will give you an output on what it predicts the text as.\n",
        "\"\"\"\n",
        "np_df = np.array([\"hello\"])\n",
        "X1= transf.transform(np_df).toarray()\n",
        "y_pred = linearOptimized.predict(X1)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "mSKm_0gR6-om"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "The confusion matrix showing the performance spread of linear SVC.\n",
        "\"\"\"\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "cm = confusion_matrix(y_test, y_preds_svc, labels=linearOptimized.classes_)\n",
        "matrix_proportions = np.zeros((3,3))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=linearOptimized.classes_)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TEZPYqQNw5hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Running tweepy to get tweets from twitter\n",
        "\"\"\"\n",
        "pip install --upgrade tweepy\n",
        "import tweepy\n",
        "auth = tweepy.AppAuthHandler()\n",
        "api = tweepy.API(auth)\n",
        "client = tweepy.Client()"
      ],
      "metadata": {
        "id": "WvTCkNuomoEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Grabbing the top 3 trends in greater london\n",
        "\"\"\"\n",
        "available_loc = api.get_place_trends(44418) # Greater London\n",
        "data = available_loc[0]\n",
        "trends = data['trends']\n",
        "trending =[]\n",
        "i=0\n",
        "for trend in trends:\n",
        "  if i > 3:\n",
        "    break\n",
        "  trendName = trend['name']\n",
        "  trending.append(trendName)\n",
        "  i+=1\n",
        "print(trending)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUx1HSrBaeUb",
        "outputId": "15288926-82fa-4c8b-ef5e-acf098ed5578"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['#JeremyVine', '#AWSSummit', 'Poland and Bulgaria']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Predicts tweets from trends\n",
        "\"\"\"\n",
        "def predictTrends(df):\n",
        "  df2 = pd.DataFrame()\n",
        "  preds = []\n",
        "  for i in range(df['tweets'].shape[0]):\n",
        "    np_df = pd.DataFrame(df['tweets'].iloc[[i]])\n",
        "    np_df = np.array(np_df)\n",
        "    X1= transf.transform(np_df[0][:]).toarray()\n",
        "    y_pred = linearOptimized.predict(X1)\n",
        "    df2 = df2.append({'Prediction': y_pred, 'Tweet': df['tweets'].iloc[i], 'Hashtag': df['hashtag'].iloc[i]}, ignore_index=True)\n",
        "    preds.append(y_pred)\n",
        "  df2['Prediction'] = df2['Prediction'].explode().astype(int)\n",
        "  return df2"
      ],
      "metadata": {
        "id": "_04nSZxw0nUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Predicts tweets from hashtags\n",
        "\"\"\"\n",
        "def predict(df):\n",
        "  df2 = pd.DataFrame()\n",
        "  preds = []\n",
        "  for i in range(df['tweets'].shape[0]):\n",
        "    np_df = pd.DataFrame(df['tweets'].iloc[[i]])\n",
        "    np_df = np.array(np_df)\n",
        "    X1= transf.transform(np_df[0][:]).toarray()\n",
        "    y_pred = linearOptimized.predict(X1)\n",
        "    df2 = df2.append({'Index': i,'Prediction': y_pred, 'Tweet': df['tweets'].iloc[i]}, ignore_index=True)\n",
        "    preds.append(y_pred)\n",
        "  df2['Prediction'] = df2['Prediction'].explode().astype(int)\n",
        "  df2['Index'] = df2['Index'].explode().astype(int)\n",
        "  return df2"
      ],
      "metadata": {
        "id": "iHTFBSqqLSUD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" \n",
        "Calling function to predict tweets and creating a dataframe.\n",
        "\"\"\"\n",
        "tweet_data = []\n",
        "hashtags = []\n",
        "for i in range(len(trending)):\n",
        "  for tweet in tweepy.Cursor(api.search_tweets,q=trending[i],count=1, tweet_mode='extended', lang='en').items(150):\n",
        "    tweet_data.append(tweet.full_text)\n",
        "    hashtags.append(trending[i])\n",
        "\n",
        "hashtagdf = pd.DataFrame(hashtags, columns=['hashtag'])\n",
        "tweetdf = pd.DataFrame(tweet_data, columns=['tweets'])\n",
        "df = pd.concat([tweetdf, hashtagdf], axis=1)\n",
        "df2 = predictTrends(df)"
      ],
      "metadata": {
        "id": "aAe9gOiMbcKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The mean score of predictions for the trends\n",
        "\"\"\"\n",
        "hateInHashtag = pd.DataFrame()\n",
        "hateInHashtag = df2['Prediction'].groupby(df2['Hashtag']).mean()\n",
        "hateInHashtag.head()"
      ],
      "metadata": {
        "id": "GTSsxZIcKTK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the model on Twitter tweets to see performance and spread of predictions."
      ],
      "metadata": {
        "id": "3vjFp0b7B-WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets,q='#WillSmith',count=500, tweet_mode='extended', lang='en').items(500):\n",
        "    tweet_data.append(tweet.full_text)\n",
        "tweetdf = pd.DataFrame(tweet_data, columns=['tweets'])\n",
        "df2 = predict(tweetdf)\n",
        "df2.groupby(['Prediction']).sum().plot(kind='pie', subplots=True, title='Will Smith',autopct='%1.0f%%')\n"
      ],
      "metadata": {
        "id": "ezk19tFvzlDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets,q='#CaitlynJenner',count=500, tweet_mode='extended', lang='en').items(500):\n",
        "    tweet_data.append(tweet.full_text)\n",
        "tweetdf = pd.DataFrame(tweet_data, columns=['tweets'])\n",
        "df2 = predict(tweetdf)\n",
        "df2.groupby(['Prediction']).sum().plot(kind='pie', subplots=True, title='Caitlyn Jenner',autopct='%1.0f%%')\n"
      ],
      "metadata": {
        "id": "jWSDQAjr6s0H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets,q='#Trump',count=500, tweet_mode='extended', lang='en').items(500):\n",
        "    tweet_data.append(tweet.full_text)\n",
        "tweetdf = pd.DataFrame(tweet_data, columns=['tweets'])\n",
        "df2 = predict(tweetdf)\n",
        "df2.groupby(['Prediction']).sum().plot(kind='pie', subplots=True, title='Trump',autopct='%1.0f%%')\n"
      ],
      "metadata": {
        "id": "rwrIFXeKgPKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets,q='#Biden',count=500, tweet_mode='extended', lang='en').items(500):\n",
        "    tweet_data.append(tweet.full_text)\n",
        "tweetdf = pd.DataFrame(tweet_data, columns=['tweets'])\n",
        "df2 = predict(tweetdf)\n",
        "df2.groupby(['Prediction']).sum().plot(kind='pie', subplots=True, title='Biden',autopct='%1.0f%%')\n"
      ],
      "metadata": {
        "id": "4KU5iCENgQZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets,q='#Ramadan',count=500, tweet_mode='extended', lang='en').items(500):\n",
        "    tweet_data.append(tweet.full_text)\n",
        "tweetdf = pd.DataFrame(tweet_data, columns=['tweets'])\n",
        "ramadan = predict(tweetdf)\n",
        "ramadan.groupby(['Prediction']).sum().plot(kind='pie', subplots=True, title='Ramadan',autopct='%1.0f%%')"
      ],
      "metadata": {
        "id": "LXZKXnq2ixqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweet_data = []\n",
        "for tweet in tweepy.Cursor(api.search_tweets,q='#Terrorists',count=500, tweet_mode='extended', lang='en').items(500):\n",
        "    tweet_data.append(tweet.full_text)\n",
        "tweetdf = pd.DataFrame(tweet_data, columns=['tweets'])\n",
        "df2 = predict(tweetdf)\n",
        "df2.groupby(['Prediction']).sum().plot(kind='pie', subplots=True, title='Terrorists',autopct='%1.0f%%')"
      ],
      "metadata": {
        "id": "zbdcPT7sizzu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}